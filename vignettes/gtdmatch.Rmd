---
title: "Matching GTD events to news articles from The Guardian"
author: "by Kasper Welbers"
date: "2019-08-14"
output:
  pdf_document:
    toc: true
toc_depth: 2
---
  
```{r, include=FALSE}
options(digits=3)
library(knitr)
```


# Development note

This is currently under development, and uses the development version of RNewsflow. 

```{r, eval=F}
devtools::install_github('kasperwelbers/RNewsflow')
```

# Introduction


```{r, eval=T}
library(restecode)
```


This file demonstrates the method used for matching the terrorist events in the Global Terrorism Database to news articles. Both the GTD data and the articles from the Guardian used in the paper [@@] are available online, and instructions are included for replicating the analysis.[^1]

[^1]: It is possible that results differ if the GTD data or Guardian data has changed. We do not include the original data due to copyright.

It should be possible to perform this vignette step-by-step, but it will take a while to download the required data, and some steps take a while to compute. We therefore first make a subdirectory (by default in the current working directory) to download the data and prepare the text corpora. 

```{r, eval=F}
dir.create('GTD_matching')
```

# Downloading and preprocessing the data

## GTD data

The GTD data can be downloaded (for free) from the [GTD website](https://www.start.umd.edu/gtd/contact/) after filling out a form with basic usage information. The full data is provided as an .xlsx file, which can be read into R with the `read.xlsx` function from the `openxlsx` package.

```{r, eval=F}
library(openxlsx)

## (change the filename accordingly)
gtd = read.xlsx(xlsxFile = '~/Downloads/globalterrorismdb_0718dist.xlsx')
```

The data can now be preprocessed into a Document Term Matrix (DTM). Here we use the `dfm` (document feature matrix) format from the `quanteda` package. The function `prepare_gtd` is provided in this package to take all the necessary steps. Here we prepare the data, using the fromdate argument to only include events after 2000-01-01.

```{r, eval=F}
gtd = prepare_gtd(gtd, fromdate = '2000-01-01', todate='2015-12-31')

## save to backup folder
saveRDS(gtd, 'GTD_matching/gtd_dtm.rds')
```

## The Guardian data

The Guardian has an API that provides free access to full texts. To access this API you first need to [register for an API key](https://bonobo.capi.gutools.co.uk/register/developer).

```{r}
api.key = "[paste your own api key here]"
```

We can access the API through the `GuardianR` package. For convenience we have made a wrapper function that downloads the data in batches to a given folder, and then creates the DTM when finished. This prevents having to download the data twice, which can take quite a while. The function can also be interupted. As long as the same query and from/to date are used, it will resume from the last downloaded batch. 

The `download_guardian` function requires a query, or a character vector of query terms. For matching the GTD data we provide a list of terrorism query terms. The list is intentionally very broad. The goal is to have high recall (i.e. find all events that mention terrorist events) and we are not that concerned about precision (i.e. finding only events that mention terrorist events) in the data collection phase.

The following code downloads about 500k articles, and will take about a good day to finish. 

```{r, eval=F}
gua = download_guardian(terrorism_terms, api.key, 
                        fromdate = '2000-01-01', todate='2015-12-31', 
                        path = 'GTD_matching')

## Here we only use the articles from publication "The Guardian" (+/- 160,000)
gua = gua[gua$publication == 'The Guardian',]
```

The data can now be preprocessed into a DTM with the `prepare_guardian` function. Here we also have the `first_n_words` arguments, which can be used to only include the first `n` words of each articles in the DTM. We used n = 200 in our analysis, assuming that articles that cover new terrorist events mention the event early due to the inverted pyramid structure of news (i.e. core message on top). This also reduces biases due to article length.

```{r, eval=F}
gua = prepare_news(gua, doc_col='id', date_col = 'webPublicationDate', 
                   text_cols = c('headline','body'), 
                   docvars = c('headline','webUrl','sectionName'), 
                   first_n_words = 200)

## save to backup folder
saveRDS(gua, 'GTD_matching/gua_dtm.rds')
```

# Matching the GTD events to the news articles

Our task is to find out whether a news article covers a **new** terrorist attack (i.e. that happened only recently), and if so match it to the corresponding GTD event. To do this, we measure whether news articles are sufficiently similar to GTD events that occured within seven days before the article publication. 

We measure the similarity of a GTD event and news article by looking at overlapping words and word combinations, weighted for how informative these words and word combinations are for distinguishing events. We adopt an established approach for measuring document similarity, in which documents are positioned in a vector space based on the occurence of word and word combinations, and the similarity of their positions is calculated. 

In this section we describe additional preprocessing steps for creating a good (and memory-efficient) vector space, and our method for calculating the similarity. The required functions are published in the RNewsflow package that we developed, and that we have updated for this event matching task.

```{r, eval=F}
library(RNewsflow)
```

As input we'll use the DTMs created above. Here we load the DTM's from the backup folder.

```{r, eval=F}
gtd = readRDS('GTD_matching/gtd_dtm.rds')
gua = readRDS('GTD_matching/gua_dtm.rds')
```

## additional preprocessing for better matching

We use several additional preprocessing steps to prepare the DTMs for the similarity calculation. While some techniques have more state-of-the-art alternatives that might yield better results on the matching task, we have purposefully restricted ourselves to methods that can be performed directly within R and with DTMs as input.  

### Cluster terms with similar spelling

In a DTM, terms that have very similar spelling are nonetheless regarded as completely separate columns. To some extent, this is accounted for by preprocessing tech"niques such as stemming (applied in our case) or lemmatization. For instance, with stemming, "attacking", "attack" and "attacks" are all transformed to "attack". However, some words--in particular namems--can have different spellings, such as "Gaddafi" or "Gadhafi". Moreover, some datasets have spelling mistakes. In the GTD data, there are quite many spelling mistakes and slightly different spellings for words, often related to differences between countries (though the prescribed language is English). 

We therefore first use a technique for clustering words with similar spelling together as single columns. The steps are as follows. We first extract all the unique column names (i.e. words) from the gtd and news DTMs, which constitutes our *vocabulary*. We then apply the `term_char_sim` function to calculate which terms are sufficiently similar. By default, `term_char_sim` looks at overlapping tri-grams (i.e. three consequtive characters) with some additional restrictions.

```{r, eval=F}
voc = union(colnames(gtd), colnames(gua))  
simmat = term_char_sim(voc)                
```

The result is an adjacency matrix of the words in the vocabulary, which is 1 if words are sufficiently similar to be merged. We can now use this matrix to cluster all similar terms together, for which we provide the `term_union` function. Note that words are also clustered together if they are related indirectly. That is, if A and B are related, and B and C are related, Then a cluster ABC is created even if A is not related to C.   

```{r, eval=F}
gtd = term_union(gtd, simmat)               
gua = term_union(gua, simmat)               
## (be carefull not to run this twice, because it overwrites gtd/gua)
```

This clustering approach is intentionally greedy. The vector space that we're creating is VERY sparse (as detailed below), so it is a big problem if the same concept (e.g., person, location, verb) has different spellings, while the chances that a different but similarly spelled concept causes a false positive are slim.[^2] Still, it is always good practice to manually check how the clustering performs by looking over some of the new columns. The new column names contain all unique terms, connected with the | (OR) operator.

[^2]: Note that later on we will weight the DTM based on overall term frequencies, so terms in very greedy clusters with and/or with common words will mostly become less informative, rather than result in false positives.  

```{r, eval=F}
cnames = colnames(gua)    
head(cnames, 20)          
```

Here we make a second backup. This is not strictly necessary, but the next steps might crash R if you do not have enough free memory.

```{r, eval=F}
saveRDS(gua, 'GTD_matching/gua_dtm_2.rds')
saveRDS(gtd, 'GTD_matching/gtd_dtm_2.rds')
```

### Compute term combinations

For our matching task, we want to use the features (i.e. terms) that are most informative, i.e. that most clearly distinguish events. Very common terms do not tell us much about whether news articles cover the same event as described in a GTD entry. However, some terms such as "London" and "attack" are not very informative on their own, but very informative combined: the London attacks. Simple vector space calculations of similarity do not take into account that the combination of "London" and "attack" together in the same article is more informative than the sum of their individual information. 

We therefore provide a function to add term combinations to the DTM. This is not a common thing to do in vector space model approaches for calculating document similarity. A major downside of the technique is that it greatly increases the number of unique terms in the DTM (a quadratic increase) which is often not feasible due to memory restrictions. Here we can pull it off by imposing a few assumptions regarding our specific matching problem and filtering the term combinations accordingly. 

To give an example, we no longer take into account whether document are similar in their frequency distribution of common articles (e.g., the, it, is). That is alright, because we are not interested in how similar the documents are written, only in whether they both contain a distinguishing feature (e.g., bomb & London, the name of an abducted person).

assumptions:

* We drop all columns of which the document probability (docprob) is higher than 0.01. That is, we ignore all terms and term combinations that occur in more than 1/100 documents. 
* We only keep combinations if terms if their observed frequency is more than 1.2 times the expected frequency. The expected frequency is the number of times we would expect two terms to co-occur based on sheer probability. We are not interested in term combinations that only occur due to sheer probability of co-occurence.

Note that these values can easily be altered, and which values are best for different use cases is an open question. We have chosen the current values based on common sense and experimentation. As discussed below, there is a good way to get quick validation measures for our type of event matching task, and this can be used to get good estimates (though beware of overfitting). 

```{r, message=F}
library(RNewsflow)
gua = readRDS('GTD_matching/gua_dtm_2.rds')
gtd = readRDS('GTD_matching/gtd_dtm_2.rds')
sf = create_queries(gtd, gua, max_docprob = 0.01, min_obs_exp = 1.2,
                    min_docfreq = 1, use_dtm_and_ref = F, verbose=T, weight = 'tfidf')
```

The output of `create_queries` is a list with the revamped gtd and gua DTMs, now called `query_dtm` and `ref_dtm` (reference). The query dtm is in this case the gtd data. Both DTMs contain the same columns. For the `query_dtm` the values are weighted using a tfidf weighting scheme (as specified in `create_queries`) and normalized so that the max value is 1. For calculating the idf only the document frequency scores of the reference dtm (i.e. news articles) are used, because we do not want to weigh down common terrorism query terms in the GTD. The values in the `ref_dtm` are binary: does the term or term-combination occur in a document?

The reason for this way of formatting the data is that we can now sum up the weights of the query terms (GTD) that occur in documents in the reference DTM (Guardian). In the `newsflow_compare` function that we use below for calculating the similarity scores for GTD and Guardian documents, we provide the `query_lookup` measure. This is an assymetric similarity score that is not normalized by the total sum of query terms or the sum of terms in the reference documents. This is important, because we are only interested in whether at least a minimum amount of weighted query terms occur.  


### Performing the comparison

The `newsflow.compare` function calculates document similarity scores for a variety of measures. Here we use the special `query_lookup` function that is designed for the output of the `create_queries` function.

Essentically, the `newsflow.compare` function is a specialized application of matrix multiplication of the query and reference DTMs. It has several important benefits, but two are of particular importance for the task of matching event data to news articles.

* Events and news occur over time. We can leverage this to only compare events and news within a given time window. For large datasets this is critical, because it greatly reduces the number of comparisons. In our current case there are 87,338 GTD events and 161,049 Guardian articles, amounting to 14065697562 comparisons. Even if time is not an issue, you would need a huge amount of memory. The `newsflow.compare` function uses a custom built matrix multiplication algorithm that incorporates an efficient sliding window approach. The `hour.window` function takes a numeric vector of length two that specifies the left and right borders of the window.
* Even with fewer comparisons, the total number of comparisons quickly amounts to a number of non-zero results that would require a huge amount of memory. For our current task we can set a threshold for the minimum similarity score, which cuts of the vast majority of these results. The `newsflow.compare` function performs the calculation of the similarity measure and applies the given threshold (`min.similarity`) within the matrix multiplication algorithm. 

The first two arguments of the `newsflow.compare` function are the two DTMs that are to be compared, in our case the `query_dtm` (GTD) and `ref_dtm` (Guardian). We also specify the time window in hours. Although we are only interested in matches within seven days after the GTD event, we also look for all matches within a window of 14 days before the event. This allows us to do a preliminary validity check, as discussed below. The `min.similarity` of 1 is a very low threshold, as shown in the validation analysis, to have a high recall. With `return_as` we specify that we want he results as an edgelist, which is a data.frame in which each row is a match, with columns indicating the GTD event id, Guardian article id, weight of the match, and the hour difference. As the measure we use the `query_lookup` as discussed above.  

```{r}
g = newsflow.compare(sf$query_dtm, 
                     sf$ref_dtm, 
                     hour.window = c(-14*24,7*24), min.similarity=1, 
                     return_as = 'edgelist',
                     measure = 'query_lookup', verbose=T)
```

### Validation 1: inspect matches over time

To get a first indication of the validity of the matches, we can look at the histogram of the hour differences. In particular, we can look at the number of matches before the event to get an estimate of the number of false positives. That is, if a GTD event matches a news article that was published before the actual event, we can be certain that the match is a false positive. Here we plot three histograms for different similarity weight thresholds to see how this affects the matches.

```{r, fig.width = 6, fig.height = 3, fig.align="center"}
par(mfrow=c(1,3))  ## plot side by side
for (thres in c(5,10,20)) 
  x = hist(g$hourdiff[g$weight > thres], right = F, main='', xlab='hour difference')
```

In all three cases we see a peak after the an hour difference of zero, which indicates that the algorithm picks up true positives. However, we also see that there are false positive, especially for the lowest threshold. This is a good sign, because it means that we get a better precision if we increase the threshold. If we increase the threshold to a point where there are only a few matches before the zero hour difference point, we can be confident that we have a high precision.

However, if our threshold is too high, this will greatly harm the recall. If we look at the second and third histogram, we see that the peak decreases from to 4832 to 1563. While a portion of this decrease is likely to be due the loss of false positives, it is very likely that we have lost many true positives as well. 

If we assume that the probability of false positives is the same for matches before and after the event, we can calculate a rough estimate of the precision and recall. While the exact estimates are not very accurate, we can them to compute which similarity threshold gives the highest F1 score. The estimation works as follows.

#### Estimating P/R

Let all possible event-to-news pairs within the comparison window be denoted as $N$, which we split into pairs where the news is published before the event $N_{before}$ and news published after the event $N_{after}$. Based on a matching algorithm we then count the matches $N$, which we also split into matches before the event $M_{before}$ and matches after the event $M_{after}$. We can now calculate the probability of false positives as:

$$P(FP_{before}) = \frac{M_{before}}{N_{before}}$$

Based on the assumption that this probability is the same for matches after the event, we can estimate the number of false positives $FP_{after}$ in matches after the event.

$$FP_{after} = P(FP_{before}) \times N_{after}$$

Now, the difference between the observed matches and estimated false positives gives the estimated true positives $\hat{TP}$, and so we can estimate the precision $\hat{P}$.

$$\hat{TP} = M_{after} - FP_{after}$$
$$\hat{P} = \frac{\hat{TP}}{M_{after}} \times 100$$

To estimate the recall, we first use our lowest similarity threshold to estimate $\hat{TP}$ for a set of matches of which we are confident that the recall is close to 100. We then use this estimate as the upper limit $TP_{limit}$. We can then calculate the recall estimate $\hat{R}$ as the percentage of this limit for the results of higher thresholds.  

$$\hat{R} = \frac{\hat{TP}}{TP_{limit}} \times 100$$

Finally, the estimated F1 $\hat{F1}$ is the harmonic mean of the $\hat{P}$ and $\hat{r}$.

$$\hat{F1} = \frac{\hat{P} \times \hat{R}}{\hat{P} + \hat{R}}$$

The following function estimates the precision, recall and F1 scores for a range of similarity thresholds. 

```{r, fig.width = 6, fig.height = 3}
estimate_validity(g)
```

As an initial validation, this methods shows that the matching algorithm is able to identify correct matches. It provides a rough but usefull indication for what could be a good weight threshold. 

### Validation 2: Gold Standard

As a second method of validation, we therefore also created a gold standard. We manually coded 500 random articles from The Guardian between 2000 and 2015 for the broad terrorism query. We first looked whether the a news article covered a `new` terrorist event. If this was the case, all GTD events within 7 days before the article publication date and that took place in the same country as mentioned in the article were manually checked for a match. 

We found that 26 out of the 500 articles matched with a GTD event. Since the GTD documents each event separately, some articles matched multiple GTD events. In total, 130 matches were found. The gold standard data is included in the `restecode` package, under the name `gold_matches`.

The `pr_plot` function uses this data to calculate the precision, recall and F1 (weighted average of precision and recall) scores. The data frame `g` is subsetted so that it only includes matches of GTD events to news articles within 7 days after the event, and only includes news articles that are in the gold standard (N = 500). The precision, recall and F1 scores are calculated separately for the article level (does the article have at least one GTD match) and match level (each individual match of an article to a GTD event). Furthermore, the scores are calculated for different weight thresholds.

```{r, fig.width = 7, fig.height = 4}
pr = gtd_pr(g)
```

The results are promising. 

If a threshold between 

If we look at the output of `gtd_pr`, that is assigned to `pr`, we can see the exact results per threshold. The threshold with the highest F-score is 


### Improving matches with postprocessing

Given the results of `newsflow_compare`, we can still apply certain postprocessing steps to improve the results. 
Note that above te assigned the result of `gtd_pr` to `pr`. We'll use this below to see whether and how strongly the postprocessing improves the results.


#### Postprocessing 1: filtering on country

The matching algorithm already uses location information for calculating the similarity score. However, we can more strictly decide that a GTD event and news article need to match on the country level. In the current data preparation functions (`prepare_gtd` and `prepare_news`) we have added the `geo` column to the DTM document variables using the `newsmap` package and an adjusted country dictionary based on the locations used in the GTD. For the GTD data this is a two-letter code for the country[^3].

[^3]: If the data is not created with the `prepare_gtd` or `prepare_news` functions, you could use the `geo_tags` function to create the geo tags.

```{r}
head(attr(g, 'from_meta')[, c('geo','country','city')])
```

For the news data the country is harder to determine, because a news article can mention other countries besides the country in which a terrorist event occured. Therefore, the `geo` column here contains the three top-ranked country matches per document.

```{r}
head(attr(g, 'to_meta')[, c('geo')])
```

The `match_geo` function uses this information to determine whether the two documents in a match also match on their geo codes. 

```{r}
g$geomatch = match_geo(g, sf$query_dtm, sf$ref_dtm)
```

The `geomatch` column that has nog been added is a logical vector. We can use this to repeat the 

```{r, fig.width = 7, fig.height = 4}
pr_geomatch = gtd_pr(g[g$geomatch,], hourdiff_range = c(0, 24*7), weight_range=c(1,30), steps=30)
```

Eyeballing the plotted results shows that the precision increases and recall slightly decreases. We can look at this more specifically by comparing the `pr` and `pr_geomatch`. The `compare_pr` function 

```{r, fig.width = 7, fig.height = 4}
gtd_compare_pr(pr, pr_geomatch)
```

The increase in precision is in particular strong in the weight range of 2 to 10. This is an imporant area to improve the precision because the recall is high here. If the goal is to obtain matches with high precision and recall, the number of matches that would require manual correction decreases substantially.

```{r}
sum(g$weight > 10)
sum(g$weight > 10 & g$geomatch)
```


#### postprocessing 2: clustering of GTD events

Still to be implemented and tested. Should increase recall, but cost to precision might be too high. 

Terrorist attacks often encompass multiple events, such as multiple bombing targets on the same day. The GTD documents these events separately, but news media often report on a string of connected attacks. In some cases the number of separate events is rather large. In our gold standard, one news article matches 40 GTD events, and another matched 21 events. To better match connected attacks, we cluster the GTD data. If a GTD event in a cluster matches a news article, we then also match all other events in the cluster.


